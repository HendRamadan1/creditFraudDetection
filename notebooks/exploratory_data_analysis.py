# -*- coding: utf-8 -*-
"""exploratory_data_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d4Ph7YVrrh8CNhUIcrU60FbkarkVmzLL

# Context
It is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase.

# Content
The dataset contains transactions made by credit cards in September 2013 by European cardholders.
This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.

It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, … V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-sensitive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.

Given the class imbalance ratio, we recommend measuring the accuracy using the Area Under the Precision-Recall Curve (AUPRC). Confusion matrix accuracy is not meaningful for unbalanced classification.

# problem Statement

The problem statement chosen for this project is to predict fraudulent credit card transactions with the help of machine learning models.

In this project, you will analyse customer-level data which has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group.

The dataset is taken from the Kaggle website and it has a total of 2,84,807 transactions, out of which 492 are fraudulent. Since the dataset is highly imbalanced, so it needs to be handled before model building
"""

#import libraries
import pandas as pd
import numpy as np
df = pd.read_csv("../src/data/raw/creditcard.csv")

df.head(10)

df["Time"].nunique()

df.shape

# duplicates in data
print(sum(df.duplicated()))

## check duplicated value
from rich import print
vals = {"duplicates" : sum(df.duplicated()), "total rows" : 284807}
import matplotlib.pyplot as plt

plt.bar(vals.keys(), vals.values());
print(f"{(sum(df.duplicated())/284807)*100} is the percent of duplicate data")

df.drop_duplicates(inplace=True)

df.shape

df["Class"].sum()

df.isna().sum()

"""## EDA

##  Distribution of Key Features

Before diving into modeling, it’s important to understand how key numerical features are distributed. This can help detect skewness, outliers, and the need for scaling.

Below, we visualize the distribution of the two main features:

- **Transaction Amount (`Amount`)**
- **Transaction Time (`Time`)**
"""

import seaborn as sns
fig, ax = plt.subplots(1, 2, figsize=(18,4)) ## distribution of time and amount
#! axis 0 is left, the latter is right

amount_val = df['Amount'].values
time_val = df['Time'].values

sns.distplot(amount_val, ax=ax[1], color='r')
ax[1].set_title('Distribution of Transaction Amount', fontsize=14)
ax[1].set_xlim([min(amount_val), max(amount_val)])

sns.distplot(time_val, ax=ax[0], color='b')
ax[0].set_title('Distribution of Transaction Time', fontsize=14)
ax[0].set_xlim([min(time_val), max(time_val)])



plt.show()

"""### Observations:

- The `Amount` feature appears to be **right-skewed**, indicating the presence of many small transactions and few large ones.
- The `Time` feature does not follow a clear pattern but may reflect **periodic trends** depending on time of day or other operational factors.
- These insights are useful for later preprocessing steps, such as normalization or feature engineering.

"""

import plotly.express as px
d = df['Class'].value_counts().reset_index(name='Count') ## show distrubution of target  varaible

fig = px.pie(d,values='Count',names=['not fraud',' fraud'],hole=0.4,opacity=0.6,
            color_discrete_sequence=["#0B0A09","#FF7676"],
             labels={'label':'Class','Class':'No. Of Samples'})

fig.add_annotation(text='We can resample the data<br> to get a balanced dataset',
                   x=1.2,y=0.9,showarrow=False,font_size=12,opacity=0.7,font_family='monospace')
fig.add_annotation(text='Class',
                   x=0.5,y=0.5,showarrow=False,font_size=14,opacity=0.7,font_family='monospace')

fig.update_layout(
    font_family='monospace',
    title=dict(text='Q. How many samples of Credit Card  are  not fraud ?',x=0.47,y=0.98,
               font=dict(color='#000000',size=20)),
    legend=dict(x=0.37,y=-0.05,orientation='h',traceorder='reversed'),
    hoverlabel=dict(bgcolor='white'))

fig.update_traces(textposition='outside', textinfo='percent+label')

fig.show()

"""Only 0.17% fraudulent transaction out all the transactions. The data is highly Unbalanced. Lets first apply our models without balancing it and if we don’t get a good accuracy then we can find a way to balance this dataset. But first, let’s implement the model without it and will balance the data only if needed."""

###realtionship between time and amount
plt.figure(figsize=(12,6))
sns.scatterplot(data=df,x='Time',y="Amount",hue="Class",palette="Set1")
plt.xlabel("Time(seconds)")
plt.title("Time vs Transaction Amount ")
plt.ylabel("Transaction Amount")
plt.tight_layout()
plt.show()

"""

- **Legitimate transactions (Class = 0, shown in red)** dominate the dataset, especially for small amounts.
- **Fraudulent transactions (Class = 1, shown in blue)** are fewer and more scattered, but may cluster around certain time periods or specific amount ranges.
- There is no strong linear relationship between time and amount, but some separation is visible between the two classes, which might help in classification.

Further analysis like **feature engineering based on time intervals** (e.g., time of day, peak hours) might improve model performance.
"""

plt.figure(figsize=(30,10))
sns.heatmap(df.corr(),annot=True)

"""

- Most features have very low correlation with the target variable `Class`, indicating that fraudulent behavior is not linearly separable based on single features.

- Some features (like `V14`, `V10`, `V17`) may show moderate negative or positive correlation with `Class` — these could be potentially informative for classification.

- Features such as `Amount` and `Time` show weak correlation with `Class`, suggesting that additional preprocessing or feature engineering may be necessary.

Note: Low correlation does not mean a feature is not useful — non-linear models (like tree-based algorithms) can capture complex relationships that correlation alone cannot reveal.
"""

### Positive corrlation

f, axes = plt.subplots(ncols=4, figsize=(20,4))

#s with our Class (The lower our feature value the more likely it will be a fraud transaction)
sns.violinplot(x="Class", y="V11", data=df, palette='Set1', ax=axes[0])
axes[0].set_title('V11 vs Class')

sns.violinplot(x="Class", y="V8", data=df, palette='Set1', ax=axes[1])
axes[1].set_title('V8 vs Class')


sns.violinplot(x="Class", y="V4", data=df, palette='Set1', ax=axes[2])
axes[2].set_title('V4 vs Class')


sns.violinplot(x="Class", y="V2", data=df, palette='Set1', ax=axes[3])
axes[3].set_title('V2 vs Class')
plt.show()

"""
#### Features with Positive Correlation:
The following features tend to have higher values in fraudulent transactions:

- `V11`
- `V8`
- `V4`
- `V2`

These features also show noticeable shifts in distribution and can contribute meaningfully to the predictive model.

Visual inspection confirms the statistical correlation values and supports selecting these features for modeling.
"""

### Negative corrlation

f, axes = plt.subplots(ncols=4, figsize=(20,4))

#s with our Class (The lower our feature value the more likely it will be a fraud transaction)
sns.violinplot(x="Class", y="V17", data=df, palette='Set1', ax=axes[0])
axes[0].set_title('V17 vs Class')

sns.violinplot(x="Class", y="V14", data=df, palette='Set1', ax=axes[1])
axes[1].set_title('V14 vs Class')


sns.violinplot(x="Class", y="V12", data=df, palette='Set1', ax=axes[2])
axes[2].set_title('V12 vs Class')


sns.violinplot(x="Class", y="V10", data=df, palette='Set1', ax=axes[3])
axes[3].set_title('V10 vs Class')
plt.show()

"""
#### Features with Negative Correlation:
The following features tend to have lower values in fraudulent transactions:

- `V17`
- `V14`
- `V12`
- `V10`

These features show a clear separation in distribution between fraud and non-fraud classes, indicating that they could be valuable for classification."""

from sklearn.preprocessing import RobustScaler
rs = RobustScaler()
df[["Amount","Time"]] = rs.fit_transform(df[["Amount","Time"]])

df[["Amount","Time"]].head()

#check for outliers
df.plot(kind='box',subplots=True, figsize=(15, 10), color="#578BEC",layout=(7,5));

##most Feature have stronge realtionship with target
target = "Class"                                # <‑‑ change if needed
corr_to_target = (df.corr(numeric_only=True)[target]
                  .drop(target)                     # remove self‑correlation
                  .abs()
                  .sort_values(ascending=False))

corr_to_target.head(15).plot.barh(figsize=(6, 7));

# from imblearn.under_sampling import RandomUnderSampler
# rus = RandomUnderSampler(random_state=0)
# X_resampled, y_resampled = rus.fit_resample(X, y)

df['Class'].value_counts()

from sklearn.metrics import classification_report,accuracy_score,confusion_matrix,f1_score

"""## undersampling"""

fraud=df.loc[df['Class']==1]
not_fraud=df.loc[df['Class']==0][:473]
normal_distribuation_df=pd.concat([fraud,not_fraud])
df=normal_distribuation_df.sample(frac=1,random_state=42)

df.head()

##Balance class
sns.countplot(data=df,x='Class')

# Separate X (features) and y (target)
X = df.drop(columns=["Class"])
y = df["Class"]

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1)

X_train.shape

y_train.shape

####Logistic Regression
from sklearn.linear_model import LogisticRegression
LR=LogisticRegression(C=0.01,penalty='l2',solver='lbfgs')
LR.fit(X_train,y_train)
y_pred2=LR.predict(X_test)

print(f"acuuuracy for train model is {LR.score(X_train,y_train)}")
print(f"acuuuracy for test model is {LR.score(X_test,y_test)}")

acc=accuracy_score(y_test,y_pred2)
f1=f1_score(y_test,y_pred2,average='weighted')
print("Accuracy:", acc)
print("F1 Score:", f1)

cals_report=classification_report(y_test,y_pred2)

print(f'Accuracy Score: {acc:.2f}')
print(f'classification report:{cals_report} ')
print('_' * 50)
print()

cm = confusion_matrix(y_test, y_pred2)
plt.figure(figsize=(7,5))
sns.heatmap(cm, annot=True, cmap='PuBu')
plt.title(f'Logistic Regression- Confution Metrics', fontsize=15)

from sklearn.neural_network import MLPClassifier

mlp = MLPClassifier(hidden_layer_sizes=(50,50), max_iter=1000, random_state=42)
mlp.fit(X_train, y_train)
y_pred=mlp.predict(X_test)

acc=accuracy_score(y_test,y_pred)
f1=f1_score(y_test,y_pred,average='weighted')
print("Accuracy:", acc)
print("F1 Score:", f1)

cals_report=classification_report(y_test,y_pred)

print(f'Accuracy Score: {acc:.2f}')
print(f'classification report:{cals_report} ')
print('_' * 50)
print()

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(7,5))
sns.heatmap(cm, annot=True, cmap='PuBu')
plt.title(f'MLPClassifier- Confution Metrics', fontsize=15)

from xgboost import XGBClassifier
XG=XGBClassifier()
XG.fit(X_train,y_train)

y_pred9=XG.predict(X_test)
acc=accuracy_score(y_test,y_pred9)
print(f'Accuracy Score: {acc:.2f}')

"""### Train with kfold"""

#### train with K_fold
from sklearn.model_selection import cross_val_score,KFold
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier

##models
classifiers = {
    "LogisiticRegression": LogisticRegression(),
    "KNearest": KNeighborsClassifier(),
    "Support Vector Classifier": SVC(),
    "DecisionTreeClassifier": DecisionTreeClassifier(),
    'XGBClassifier':XGBClassifier()

}

kfold=KFold(n_splits=5,shuffle=True,random_state=42)

for key,value in classifiers.items():
    value.fit(X_train,y_train)
    training_score=cross_val_score(value,X_train,y_train,cv=5)
    print("Classifiers: ", value.__class__.__name__, "Has a training score of", round(training_score.mean(), 2) * 100, "% accuracy score")

from yellowbrick.classifier import ROCAUC
from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier,RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
mod = []
cv_score=[]
model =[AdaBoostClassifier(), BaggingClassifier(), GradientBoostingClassifier(), ExtraTreeClassifier(), RandomForestClassifier()]
for m in model:
    cv_score.append(cross_val_score(m, X_train, y_train, scoring='accuracy', cv=5).mean())
    mod.append(m)
model_df=pd.DataFrame(columns=['model','cv_score'])
model_df['model']=mod
model_df['cv_score']=cv_score
model_df.sort_values(by=['cv_score'], ascending=True).style.background_gradient(subset=['cv_score'])

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt
from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier, ExtraTreesClassifier, RandomForestClassifier

models = {
    "LogisticRegression ":LR,
    "AdaBoostClassifier": AdaBoostClassifier(),
    "GradientBoosting": GradientBoostingClassifier(),
    "XGBClassifier": XGBClassifier(),
    "RandomForest": RandomForestClassifier()
}

# تدريب كل موديل
for name, model in models.items():
    model.fit(X_train, y_train)

# رسم ROC لكل موديل
plt.figure(figsize=(10, 6))

for name, model in models.items():
    y_probs = model.predict_proba(X_test)[:, 1]
    fpr, tpr, _ = roc_curve(y_test, y_probs)
    roc_auc = auc(fpr, tpr)

    plt.plot(fpr, tpr, label=f"{name} (AUC = {roc_auc:.2f})")

plt.plot([0, 1], [0, 1], 'k--')  # خط عشوائي
plt.title("ROC Curve for All Models")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend()
plt.grid(True)
plt.show()

"""### Hyper_parameter Tuning"""

from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC

model_params = {
    'logistic regression' : {
        'model': LogisticRegression(),
        'parameter' : {
            'solver': ['liblinear']
        }
    },
    'svm' : {
        'model' : SVC(),
        'parameter' : {
            'kernel' : ['rbf','linear'],
            'C' : [10,15,20]
        }
    },
    'decision tree' : {
        'model' : DecisionTreeClassifier(),
        'parameter' : {
            'criterion' : ['gini', 'entropy']
        }
    },
    'random forest' : {
        'model': RandomForestClassifier(),
        'parameter' : {
            'criterion': ['gini','entropy'],
            'n_estimators' : [50,100,150]
        }
    },
    'naive_bayes_gaussian' : {
        'model' : GaussianNB(),
        'parameter' : {}
    },
    'k nearest neighbors': {
        'model' : KNeighborsClassifier(),
        'parameter' : {
            'n_neighbors' : [5,10,15]
        }
    }
}

from sklearn.model_selection import GridSearchCV

score = []

for model_name, mp in model_params.items():
    clf = GridSearchCV(mp['model'], mp['parameter'], cv=5)
    clf.fit(X_train,y_train)
    score.append({
        'model' : model_name,
        'best_score' : clf.best_score_,
        'best_params' : clf.best_params_
    })

cc_df = pd.DataFrame(score, columns = ['model', 'best_score', 'best_params'])
cc_df